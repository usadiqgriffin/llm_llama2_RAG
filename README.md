###                                                   **Introduction**

In this project, I used a quantized version of llama2-base and PEFT/LoRA to reduce update size and help fit model into memory. 
The model is then fine-tuned on a Lightning.ai T4 GPU (16GB) RAM where it trained without any issue.

<p align="center">
<img src="https://github.com/usadiqgriffin/llm_llama2_RAG/assets/64921871/e3232b52-2865-4a37-91c7-2176c29fb30f" width="400">
</p>
