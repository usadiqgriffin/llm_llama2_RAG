In this notebook, I use a quantized version of llama2-base and PEFT/LoRA to reduce update size. 
The model is then fine-tuned on a Lightning.io T4 GPU (16GB) RAM where it trained without any issue.

![image](https://github.com/usadiqgriffin/llm_llama2_RAG/assets/64921871/e3232b52-2865-4a37-91c7-2176c29fb30f)
